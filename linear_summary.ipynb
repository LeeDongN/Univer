{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_summary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOInrCEaT1itjm26uPu/O/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongN/Univer/blob/main/linear_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npOhuOgQK0Ml"
      },
      "source": [
        "# **Transformation**\n",
        "$R^n$\n",
        "   - ## transformation( function or mappint)\n",
        "\n",
        "      - **T** : $R^n$ $\t\\rightarrow$ $R^m$ (n차원에서 m차원으로 mapping을 함)\n",
        "\n",
        "      - **T** : $R^n$ $\t\\rightarrow$ $R^m$이 linear transformation 하다면...\n",
        "\n",
        "         - aditivity, homogeneous (superposition)\n",
        "         - A에 대해 standard matrix가 unique하게 존재함\n",
        "         - A**X** = **b**\n",
        "\n",
        "            $\t\\rightarrow$ matrix와 vextor의 곱으로 나타냄.\n",
        "\n",
        "            $\t\\rightarrow$ matrix equation은 system of linear equation성질과 연결된다.\n",
        "\n",
        "      - $R^n$ : Domain , $R^m$ : Codomain\n",
        "\n",
        "      - image : T(X)가 x에 대한 image가 됨 (거울 앞에 사람(3차원)이 서있을 때, 거울에 비치는 상(2차원)이 image가 됨.)\n",
        "\n",
        "      - range : 모든 image(T(x))를 합침. (domain에 있던 값들이 실질적으로 mapping 된 값)\n",
        "\n",
        "         - range $\\subset$ codomain\n",
        "\n",
        "# **Matrix transformation**\n",
        "\n",
        "   - ## **X** $\\subset$ $R^n$, A**X**(image) $\\subset$ $R^m$ 이면 m x n matrix로 만들어짐.\n",
        "\n",
        "   - ## Matrix transformation 종류\n",
        "\n",
        "      - Reflection\n",
        "      - Contractions and expansions\n",
        "      - Shears\n",
        "      - Projections\n",
        "\n",
        "   - ## A transformation이 linear하기 위해서는...\n",
        "\n",
        "      - Additivity\n",
        "      - Homogeneous\n",
        "\n",
        "         $\\Rightarrow$ ***superpoosition principle***\n",
        "\n",
        "   - ## **T** : $R^n$ $\t\\rightarrow$ $R^m$ 일때...\n",
        "\n",
        "      - matrix A : the linear transformation T의 standard matrix\n",
        "      - matrix A는 unique 함.\n",
        "\n",
        "   - ## Theorem 11\n",
        "\n",
        "      - Let **T** : $R^n$ $\t\\rightarrow$ $R^m$이 linear transformation\n",
        "      - T가 one-to-one $\\leftrightarrow$ A**X** = **0** has only trivial solution\n",
        "\n",
        "         $\\ast$ tirvial solution을 가지진다는 것은...\n",
        "\n",
        "          $\\rightarrow$ linear independent 함.\n",
        "\n",
        "          $\\rightarrow$ **X** = **0**\n",
        "\n",
        "    - ## Theorem 12\n",
        "\n",
        "       - Let T :  $R^n$ $\t\\rightarrow$ $R^m$ be a llinear transformation and A를 T의 standard matrix라 했을 때,\n",
        "\n",
        "          - T maps  $R^n$ onto $R^m$ $\t\\leftrightarrow$ colum of A span $R^m$\n",
        "\n",
        "            $\\ast$ onto : range와 codomain이 같음\n",
        "\n",
        "            $\\Rightarrow$ span{**$a_1$**, **$a_2$**, ... , **$a_n$**}\n",
        "\n",
        "            $\\Rightarrow$ **$a_1$**, **$a_2$**, ... , **$a_n$**의 선형 조합으로 만들어지는 것(range) $\\subset$ $R^m$\n",
        "\n",
        "          - T가 one-to-one $\\leftrightarrow$ columns A가 lineary independent (trivial solution)\n",
        "\n",
        "            $\\Rightarrow$ one-to-one 이면서 onto면 역행렬 가짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRlGtiDPEh4"
      },
      "source": [
        ""
      ]
    }
  ]
}